{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanbelo/Aromatase/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pW5SNuaZYO1"
      },
      "outputs": [],
      "source": [
        "# prompt: connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) # add the force_remount parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vP6q4-1bZvd"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y5o1AKdbZy0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHo7Xn3IbZ2d"
      },
      "outputs": [],
      "source": [
        "x = pd.read_csv('data.csv', sep = \";\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqbozd1tevzN"
      },
      "outputs": [],
      "source": [
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0qbkTxMev2U"
      },
      "outputs": [],
      "source": [
        "x.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqguYMXHev40"
      },
      "outputs": [],
      "source": [
        "x1 =x[['Molecule ChEMBL ID','Smiles','Standard Type', 'Standard Relation', 'Standard Value',\n",
        "       'Standard Units']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dufOGYMJfYL9"
      },
      "outputs": [],
      "source": [
        "x1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMCtOSNKfYO0"
      },
      "outputs": [],
      "source": [
        "x1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t18hSLghfYR4"
      },
      "outputs": [],
      "source": [
        "x1['Standard Units'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU6mIwjEbZ5t"
      },
      "outputs": [],
      "source": [
        "x1['Standard Relation'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjyOGDLiiuhi"
      },
      "outputs": [],
      "source": [
        "x1['Standard Type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP_Y7lbFk4-6"
      },
      "outputs": [],
      "source": [
        "x1.sort_values('Standard Units',ascending= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKNGZY9FmcCA"
      },
      "outputs": [],
      "source": [
        "x1=x1.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7beNq0kmJnJ"
      },
      "outputs": [],
      "source": [
        "df =x1[x1['Standard Units'].str.contains('nM')]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufTR6KOXnInJ"
      },
      "outputs": [],
      "source": [
        "df['Standard Type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5k2Aob0i0k5"
      },
      "outputs": [],
      "source": [
        "df['Molecule ChEMBL ID'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnU_QtkLi0iJ"
      },
      "outputs": [],
      "source": [
        "df3=df[df['Molecule ChEMBL ID'].str.contains('CHEMBL488')]\n",
        "df3.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2C4L3aJi0fy"
      },
      "outputs": [],
      "source": [
        "df3.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGhvLKHEi0dB"
      },
      "outputs": [],
      "source": [
        "df3['Standard Value'].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYfdPRNH2Xu6"
      },
      "outputs": [],
      "source": [
        "df3=df3.sort_values('Standard Value',ascending= True)\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In2G8in_-6k2"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_tobc5C-6h3"
      },
      "outputs": [],
      "source": [
        "df=df.sort_values('Standard Value',ascending= True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5Fjrsob-6fO"
      },
      "outputs": [],
      "source": [
        "df=df.drop_duplicates(subset=['Molecule ChEMBL ID'], keep='first')\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf2GqyZY-6b_"
      },
      "outputs": [],
      "source": [
        "df =df[df['Standard Type'].str.contains('IC50')]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETQcxrBeAn5f"
      },
      "outputs": [],
      "source": [
        "df['Standard Type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_u0TDhAn2O"
      },
      "outputs": [],
      "source": [
        "# prompt: remove pIC50 and Log IC50 in the standard type\n",
        "\n",
        "df = df[~df['Standard Type'].str.contains('pIC50')]\n",
        "df = df[~df['Standard Type'].str.contains('Log IC50')]\n",
        "df['Standard Type'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2uZnUO0BhGV"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33FXZAmdBj5V"
      },
      "outputs": [],
      "source": [
        "active = df.loc[df['Standard Value']<= 100]\n",
        "active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K48h6qR1CYFW"
      },
      "outputs": [],
      "source": [
        "active.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0MHzqJqCfIl"
      },
      "outputs": [],
      "source": [
        "inactive = df.loc[df['Standard Value']>= 1000]\n",
        "inactive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIw_j0UzC6JM"
      },
      "outputs": [],
      "source": [
        "# prompt: deal with the data inbalance\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Combine active and inactive datasets\n",
        "combined_df = pd.concat([active, inactive])\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = combined_df[combined_df['Standard Value'] >= 1000]\n",
        "df_minority = combined_df[combined_df['Standard Value'] <= 100]\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Display new class counts\n",
        "print(df_upsampled['Standard Value'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1_cW1ZcJfzT"
      },
      "outputs": [],
      "source": [
        "df_upsampled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwC3DFfuJ6gr"
      },
      "outputs": [],
      "source": [
        "df = df_upsampled[~df_upsampled['Standard Units'].str.contains('ug.mL-1')]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLwyA_oYJrwh"
      },
      "outputs": [],
      "source": [
        "active = df.loc[df['Standard Value']<= 100]\n",
        "active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe7wKIx6KaJy"
      },
      "outputs": [],
      "source": [
        "inactive = df.loc[df['Standard Value']>= 1000]\n",
        "inactive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elQZ5uX1KzRa"
      },
      "outputs": [],
      "source": [
        "active['Standard Units'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-mbhCovKnVK"
      },
      "outputs": [],
      "source": [
        "inactive['Standard Units'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vd25yzYK5ly"
      },
      "outputs": [],
      "source": [
        "active = active.assign(label =1)\n",
        "inactive = inactive.assign(label =0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khSsjy_tL99Z"
      },
      "outputs": [],
      "source": [
        "active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-4PrvfEMAT5"
      },
      "outputs": [],
      "source": [
        "inactive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGD403vPMPI5"
      },
      "outputs": [],
      "source": [
        "combined = pd.concat([active, inactive], axis = 0)\n",
        "combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhIZRu1yMV4K"
      },
      "outputs": [],
      "source": [
        "combined.to_csv('aromatase_filtered.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2pfLTkYMqlK"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlisrjk_MyF5"
      },
      "outputs": [],
      "source": [
        "combined[['Smiles','label']].to_csv(\"aromatase.smi\", index=None, header =None, sep='\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXDfFdldOHTg"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4ElwxEROJgA"
      },
      "outputs": [],
      "source": [
        "! pip install rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_oVjp0xOvVw"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWuY89iQOzEI"
      },
      "outputs": [],
      "source": [
        "Draw.MolsToGridImage([Chem.MolFromSmiles(smi) for smi in combined['Smiles'].iloc[0:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmP10Y_HPIYF"
      },
      "outputs": [],
      "source": [
        "# prompt: perform some analysis in the chemical structure\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import Lipinski\n",
        "\n",
        "# Function to calculate molecular properties\n",
        "def calculate_properties(smiles):\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  if mol is not None:\n",
        "    molecular_weight = Descriptors.MolWt(mol)\n",
        "    logp = Descriptors.MolLogP(mol)\n",
        "    num_h_donors = Lipinski.NumHDonors(mol)\n",
        "    num_h_acceptors = Lipinski.NumHAcceptors(mol)\n",
        "    return molecular_weight, logp, num_h_donors, num_h_acceptors\n",
        "  else:\n",
        "    return None, None, None, None\n",
        "\n",
        "# Apply the function to the 'Smiles' column\n",
        "combined['Molecular Weight'], combined['LogP'], combined['NumHDonors'], combined['NumHAcceptors'] = zip(*combined['Smiles'].apply(calculate_properties))\n",
        "\n",
        "# Print the first few rows with the calculated properties\n",
        "print(combined[['Smiles', 'Molecular Weight', 'LogP', 'NumHDonors', 'NumHAcceptors']].head())\n",
        "\n",
        "# You can perform further analysis based on these properties,\n",
        "# such as calculating the distribution of molecular weights,\n",
        "# identifying compounds with specific properties, or\n",
        "# creating visualizations to understand the data better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl0gkcQXPIbQ"
      },
      "outputs": [],
      "source": [
        "# prompt: perform further chemical structure analysis\n",
        "\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "\n",
        "# Function to calculate additional molecular properties\n",
        "def calculate_more_properties(smiles):\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  if mol is not None:\n",
        "    tpsa = rdMolDescriptors.CalcTPSA(mol)\n",
        "    num_rotatable_bonds = Lipinski.NumRotatableBonds(mol)\n",
        "    num_rings = Lipinski.RingCount(mol)\n",
        "    num_atoms = mol.GetNumAtoms()\n",
        "    # Add more properties as needed\n",
        "    return tpsa, num_rotatable_bonds, num_rings, num_atoms\n",
        "  else:\n",
        "    return None, None, None, None\n",
        "\n",
        "\n",
        "# Apply the function to the 'Smiles' column\n",
        "combined['TPSA'], combined['NumRotatableBonds'], combined['NumRings'], combined['NumAtoms'] = zip(*combined['Smiles'].apply(calculate_more_properties))\n",
        "\n",
        "\n",
        "# Print the first few rows with the calculated properties\n",
        "print(combined[['Smiles', 'TPSA', 'NumRotatableBonds', 'NumRings', 'NumAtoms']].head())\n",
        "\n",
        "\n",
        "# Further analysis, such as:\n",
        "# - Correlation analysis between properties and activity (label)\n",
        "# - Distribution analysis of different properties for active and inactive compounds\n",
        "# - Identifying compounds with specific property ranges\n",
        "# - Visualizing the properties in scatter plots or histograms\n",
        "# - Clustering compounds based on their properties\n",
        "# - Feature selection for building a predictive model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utXLs1ATPIeG"
      },
      "outputs": [],
      "source": [
        "# prompt:  Further analysis, such as:\n",
        "# # - Correlation analysis between properties and activity (label)\n",
        "# # - Distribution analysis of different properties for active and inactive compounds\n",
        "# # - Identifying compounds with specific property ranges\n",
        "# # - Visualizing the properties in scatter plots or histograms\n",
        "# # - Clustering compounds based on their properties\n",
        "# # - Feature selection for building a predictive model\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Correlation analysis between properties and activity (label)\n",
        "correlation_matrix = combined[['Molecular Weight', 'LogP', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'NumRotatableBonds', 'NumRings', 'NumAtoms', 'label']].corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Properties and Activity')\n",
        "plt.show()\n",
        "\n",
        "# Distribution analysis of different properties for active and inactive compounds\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data=combined, x='Molecular Weight', hue='label', kde=True)\n",
        "plt.title('Distribution of Molecular Weight for Active and Inactive Compounds')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data=combined, x='LogP', hue='label', kde=True)\n",
        "plt.title('Distribution of LogP for Active and Inactive Compounds')\n",
        "plt.show()\n",
        "\n",
        "# Identifying compounds with specific property ranges\n",
        "# For example, find compounds with LogP between 2 and 5\n",
        "specific_compounds = combined[(combined['LogP'] >= 2) & (combined['LogP'] <= 5)]\n",
        "print(specific_compounds)\n",
        "\n",
        "# Visualizing the properties in scatter plots or histograms\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=combined, x='Molecular Weight', y='LogP', hue='label')\n",
        "plt.title('Scatter Plot of Molecular Weight vs LogP')\n",
        "plt.show()\n",
        "\n",
        "# Clustering compounds based on their properties\n",
        "# You can use KMeans clustering or hierarchical clustering\n",
        "from sklearn.cluster import KMeans\n",
        "X = combined[['Molecular Weight', 'LogP', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'NumRotatableBonds', 'NumRings', 'NumAtoms']]\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
        "combined['cluster'] = kmeans.labels_\n",
        "print(combined.groupby('cluster')['label'].mean()) # Check if clusters are related to activity\n",
        "\n",
        "# Feature selection for building a predictive model\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "X = combined[['Molecular Weight', 'LogP', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'NumRotatableBonds', 'NumRings', 'NumAtoms']]\n",
        "y = combined['label']\n",
        "selector = SelectKBest(f_classif, k=5) # Select top 5 features\n",
        "X_new = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(selected_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbHqicR5Q0nI"
      },
      "outputs": [],
      "source": [
        "# prompt: more deep chemical analysis\n",
        "\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from rdkit.Chem import rdmolops\n",
        "from rdkit.Chem import Lipinski\n",
        "\n",
        "# Function to calculate more detailed chemical properties\n",
        "def calculate_detailed_properties(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        # Calculate properties related to ring systems\n",
        "        # num_aromatic_rings = rdmolops.GetAromaticRings(mol) # This function does not exist\n",
        "        num_aromatic_rings = rdMolDescriptors.CalcNumAromaticRings(mol) # Use this function instead to calculate the number of aromatic rings\n",
        "        num_aliphatic_rings = rdMolDescriptors.CalcNumAliphaticRings(mol)\n",
        "        num_saturated_rings = rdMolDescriptors.CalcNumSaturatedRings(mol)\n",
        "        num_heterocycles = rdMolDescriptors.CalcNumHeterocycles(mol)\n",
        "\n",
        "        # Calculate properties related to functional groups\n",
        "        num_halogens = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() in [17, 35, 53])\n",
        "        num_nitrogens = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 7)\n",
        "        num_oxygens = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 8)\n",
        "        num_sulfurs = sum(1 for atom in mol.GetAtoms() if atom.GetAtomicNum() == 16)\n",
        "\n",
        "        # Calculate other descriptors\n",
        "        num_valence_electrons = Descriptors.NumValenceElectrons(mol)\n",
        "        fr_alkyl_halide = rdMolDescriptors.CalcNumLipinskiHBA(mol) # Example of a specific functional group count\n",
        "\n",
        "        return num_aromatic_rings, num_aliphatic_rings, num_saturated_rings, num_heterocycles, num_halogens, num_nitrogens, num_oxygens, num_sulfurs, num_valence_electrons, fr_alkyl_halide\n",
        "    else:\n",
        "        return [None] * 10  # Return a list of None values for all properties\n",
        "\n",
        "\n",
        "# Apply the function to the 'Smiles' column\n",
        "(combined['NumAromaticRings'], combined['NumAliphaticRings'], combined['NumSaturatedRings'], combined['NumHeterocycles'],\n",
        " combined['NumHalogens'], combined['NumNitrogens'], combined['NumOxygens'], combined['NumSulfurs'],\n",
        " combined['NumValenceElectrons'], combined['fr_alkyl_halide']) = zip(*combined['Smiles'].apply(calculate_detailed_properties))\n",
        "\n",
        "# Print the first few rows with the calculated properties\n",
        "print(combined[['Smiles', 'NumAromaticRings', 'NumAliphaticRings', 'NumSaturatedRings', 'NumHeterocycles',\n",
        "                'NumHalogens', 'NumNitrogens', 'NumOxygens', 'NumSulfurs', 'NumValenceElectrons', 'fr_alkyl_halide']].head())\n",
        "\n",
        "# Further analysis using these properties:\n",
        "# - Explore relationships between ring systems and activity\n",
        "# - Identify compounds with specific functional groups\n",
        "# - Analyze the influence of different atom types on activity\n",
        "# - Use these properties in machine learning models for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9cLVGkaDUKv"
      },
      "outputs": [],
      "source": [
        "# prompt: store the the above output in csv file\n",
        "\n",
        "combined.to_csv('aromatase_with_properties.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0YuHv1DPInT"
      },
      "outputs": [],
      "source": [
        "# prompt: # Further analysis using these properties:\n",
        "# # - Explore relationships between ring systems and activity\n",
        "# # - Identify compounds with specific functional groups\n",
        "# # - Analyze the influence of different atom types on activity\n",
        "# # - Use these properties in machine learning models for prediction\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Explore relationships between ring systems and activity\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='label', y='NumAromaticRings', data=combined)\n",
        "plt.title('Relationship between Number of Aromatic Rings and Activity')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='label', y='NumHeterocycles', data=combined)\n",
        "plt.title('Relationship between Number of Heterocycles and Activity')\n",
        "plt.show()\n",
        "\n",
        "# Identify compounds with specific functional groups\n",
        "halogen_compounds = combined[combined['NumHalogens'] > 0]\n",
        "print(\"Compounds with Halogens:\", halogen_compounds.shape[0])\n",
        "\n",
        "nitrogen_compounds = combined[combined['NumNitrogens'] > 0]\n",
        "print(\"Compounds with Nitrogens:\", nitrogen_compounds.shape[0])\n",
        "\n",
        "# Analyze the influence of different atom types on activity\n",
        "# You can use statistical tests or machine learning models to analyze the relationship between atom counts and activity\n",
        "# For example, you can use a t-test to compare the average number of nitrogens in active and inactive compounds\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "active_nitrogens = combined[combined['label'] == 1]['NumNitrogens']\n",
        "inactive_nitrogens = combined[combined['label'] == 0]['NumNitrogens']\n",
        "t_statistic, p_value = ttest_ind(active_nitrogens, inactive_nitrogens)\n",
        "print(\"T-test for Nitrogen count between active and inactive compounds:\")\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Use these properties in machine learning models for prediction\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Select features for the model\n",
        "features = ['Molecular Weight', 'LogP', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'NumRotatableBonds', 'NumRings', 'NumAtoms',\n",
        "            'NumAromaticRings', 'NumAliphaticRings', 'NumSaturatedRings', 'NumHeterocycles', 'NumHalogens', 'NumNitrogens',\n",
        "            'NumOxygens', 'NumSulfurs', 'NumValenceElectrons', 'fr_alkyl_halide']\n",
        "X = combined[features]\n",
        "y = combined['label']\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG3QRmWJRv74"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem.MolStandardize import rdMolStandardize # Import the required module\n",
        "\n",
        "def standardize_smiles(smiles):\n",
        "  \"\"\"Standardizes a SMILES string using RDKit.\"\"\"\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  if mol is not None:\n",
        "    # Remove stereochemistry\n",
        "    Chem.rdmolops.RemoveStereochemistry(mol)\n",
        "    # Neutralize the molecule\n",
        "    mol = rdMolStandardize.ChargeParent(mol) # Use the correct function to neutralize charges\n",
        "    # Convert to canonical SMILES\n",
        "    return Chem.MolToSmiles(mol, isomericSmiles=False)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "# Apply the function to the 'Smiles' column\n",
        "combined['Standardized_Smiles'] = combined['Smiles'].apply(standardize_smiles)\n",
        "\n",
        "print(combined[['Smiles', 'Standardized_Smiles']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSyNXSS920zr"
      },
      "outputs": [],
      "source": [
        "combined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3iWKjJ2DzD4"
      },
      "outputs": [],
      "source": [
        "combined.to_csv('aromatase_standardized.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBraNiBZEpm_"
      },
      "outputs": [],
      "source": [
        "# prompt: create a new data  from aromatase_standardized.csv, select : Filter\n",
        "# Molecule ChEMBL ID, Standarized_Smiles, label\n",
        "\n",
        "import pandas as pd\n",
        "new_df = pd.read_csv('aromatase_standardized.csv', usecols=['Molecule ChEMBL ID', 'Standardized_Smiles', 'label'])\n",
        "print(new_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWn8SNgJFaur"
      },
      "outputs": [],
      "source": [
        "new_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N68AwZbbSAje"
      },
      "outputs": [],
      "source": [
        "t1 = Chem.SmilesMolSupplier('aromatase.smi', delimiter='\\t', titleLine=False)\n",
        "t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQaSfG2ISyBu"
      },
      "outputs": [],
      "source": [
        "fp = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048) for mol in t1 if mol]\n",
        "fp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxls_1ihT5R6"
      },
      "outputs": [],
      "source": [
        "train = np.asarray(fp, dtype= np.int32)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1FF0EZUUUre"
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A58h_iKNUtov"
      },
      "outputs": [],
      "source": [
        "ids = [mol.GetProp('_Name') for mol in t1 if mol]\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gWHHW-vWnAn"
      },
      "outputs": [],
      "source": [
        "labels = np.asarray(ids, dtype = int).reshape(-1,1) # Use the built-in int instead of np.int\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx_66UcgU7Au"
      },
      "outputs": [],
      "source": [
        "dataset = np.hstack([train, labels])\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWlqX5C4U7GU"
      },
      "outputs": [],
      "source": [
        "np.save('dataset_feature', dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDnQ315SU7Jc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgyFi7S3U7M1"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(train, labels, test_size=0.25,shuffle= True,  random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hgpGkAWaN13"
      },
      "outputs": [],
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC0PvoT5xs6X"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3L2QiutpaNyc"
      },
      "outputs": [],
      "source": [
        "rf_Morgan = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_Morgan.fit(x_train, y_train.ravel())\n",
        "preds = rf_Morgan.predict(x_test)\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C20L_Z4lbaDy"
      },
      "outputs": [],
      "source": [
        " roc_auc_score(y_test, preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQVOacLsbaHG"
      },
      "outputs": [],
      "source": [
        "# prompt: other metrics\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate other metrics\n",
        "precision = precision_score(y_test, preds)\n",
        "recall = recall_score(y_test, preds)\n",
        "f1 = f1_score(y_test, preds)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnHIycRDbaKt"
      },
      "outputs": [],
      "source": [
        "# prompt: more metrics\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, preds)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "# Calculate Balanced Accuracy Score\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, preds)\n",
        "print(\"Balanced Accuracy Score:\", balanced_accuracy)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, preds)\n",
        "print(\"Cohen's Kappa Score:\", kappa)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9xm91vHdR_0"
      },
      "outputs": [],
      "source": [
        "!pip install pycm # install the missing pycm module\n",
        "from pycm import*  # import the ConfusionMatrix class from the pycm module\n",
        "cm = ConfusionMatrix(actual_vector=y_test, predict_vector=preds) # create a confusion matrix object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9BvWMchaNvU"
      },
      "outputs": [],
      "source": [
        "from pycm import ConfusionMatrix\n",
        "cm = ConfusionMatrix(y_test.reshape (-1),preds)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqcxvinRexah"
      },
      "outputs": [],
      "source": [
        "# prompt: train with other machine learning\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ... (Your existing code for data preparation and feature extraction) ...\n",
        "\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model_lr = LogisticRegression(random_state=42)\n",
        "model_lr.fit(X_train, y_train)\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# Train an SVM model\n",
        "model_svm = SVC(random_state=42)\n",
        "model_svm.fit(X_train, y_train)\n",
        "y_pred_svm = model_svm.predict(X_test)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Train a K-Nearest Neighbors model\n",
        "model_knn = KNeighborsClassifier()\n",
        "model_knn.fit(X_train, y_train)\n",
        "y_pred_knn = model_knn.predict(X_test)\n",
        "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
        "print(classification_report(y_test, y_pred_knn))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQIjPqKjfO77"
      },
      "outputs": [],
      "source": [
        "# prompt: all metrics\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "from pycm import ConfusionMatrix\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred):\n",
        "  \"\"\"Calculates various classification metrics.\"\"\"\n",
        "\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "  # Convert Pandas Series to a NumPy array\n",
        "  cm = ConfusionMatrix(actual_vector=y_true.to_numpy(), predict_vector=y_pred)\n",
        "\n",
        "  metrics = {\n",
        "      'Accuracy': accuracy,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "      'F1-Score': f1,\n",
        "      'ROC AUC': roc_auc,\n",
        "      'MCC': mcc,\n",
        "      'Balanced Accuracy': balanced_accuracy,\n",
        "      'Kappa': kappa,\n",
        "      'Confusion Matrix': cm\n",
        "  }\n",
        "\n",
        "  return metrics\n",
        "\n",
        "# Example usage (replace y_test and y_pred with your actual values)\n",
        "metrics = calculate_all_metrics(y_test, y_pred)\n",
        "for metric_name, metric_value in metrics.items():\n",
        "  print(f\"{metric_name}: {metric_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rm_qVqmfUwh"
      },
      "outputs": [],
      "source": [
        "# prompt: details aoutcome for each single model\n",
        "\n",
        "def model_details(model_name, y_true, y_pred):\n",
        "  \"\"\"Calculates and prints detailed metrics for a single model.\"\"\"\n",
        "\n",
        "  print(f\"\\n--- {model_name} Model Details ---\")\n",
        "\n",
        "  metrics = calculate_all_metrics(y_true, y_pred)\n",
        "\n",
        "  for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "\n",
        "# Example usage for each model:\n",
        "model_details(\"Random Forest\", y_test, y_pred)\n",
        "model_details(\"Logistic Regression\", y_test, y_pred_lr)\n",
        "model_details(\"SVM\", y_test, y_pred_svm)\n",
        "model_details(\"KNN\", y_test, y_pred_knn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hysK-0ALBRlF"
      },
      "outputs": [],
      "source": [
        "# prompt: perform a model for MLP, Xgboost, Decision tree et 5 more MLs\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# ... (Your existing code for data preparation and feature extraction) ...\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an MLP model\n",
        "model_mlp = MLPClassifier(random_state=42)\n",
        "model_mlp.fit(X_train, y_train)\n",
        "y_pred_mlp = model_mlp.predict(X_test)\n",
        "model_details(\"MLP\", y_test, y_pred_mlp)\n",
        "\n",
        "\n",
        "# Train an XGBoost model\n",
        "model_xgb = XGBClassifier(random_state=42)\n",
        "model_xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "model_details(\"XGBoost\", y_test, y_pred_xgb)\n",
        "\n",
        "\n",
        "# Train a Decision Tree model\n",
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "model_dt.fit(X_train, y_train)\n",
        "y_pred_dt = model_dt.predict(X_test)\n",
        "model_details(\"Decision Tree\", y_test, y_pred_dt)\n",
        "\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model_lr = LogisticRegression(random_state=42)\n",
        "model_lr.fit(X_train, y_train)\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "model_details(\"Logistic Regression\", y_test, y_pred_lr)\n",
        "\n",
        "\n",
        "# Train an SVM model\n",
        "model_svm = SVC(random_state=42)\n",
        "model_svm.fit(X_train, y_train)\n",
        "y_pred_svm = model_svm.predict(X_test)\n",
        "model_details(\"SVM\", y_test, y_pred_svm)\n",
        "\n",
        "\n",
        "# Train a K-Nearest Neighbors model\n",
        "model_knn = KNeighborsClassifier()\n",
        "model_knn.fit(X_train, y_train)\n",
        "y_pred_knn = model_knn.predict(X_test)\n",
        "model_details(\"KNN\", y_test, y_pred_knn)\n",
        "\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "model_nb = GaussianNB()\n",
        "model_nb.fit(X_train, y_train)\n",
        "y_pred_nb = model_nb.predict(X_test)\n",
        "model_details(\"Naive Bayes\", y_test, y_pred_nb)\n",
        "\n",
        "\n",
        "# Train an AdaBoost model\n",
        "model_ada = AdaBoostClassifier(random_state=42)\n",
        "model_ada.fit(X_train, y_train)\n",
        "y_pred_ada = model_ada.predict(X_test)\n",
        "model_details(\"AdaBoost\", y_test, y_pred_ada)\n",
        "\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "model_gb = GradientBoostingClassifier(random_state=42)\n",
        "model_gb.fit(X_train, y_train)\n",
        "y_pred_gb = model_gb.predict(X_test)\n",
        "model_details(\"Gradient Boosting\", y_test, y_pred_gb)\n",
        "\n",
        "\n",
        "# Train a Bagging model\n",
        "model_bag = BaggingClassifier(random_state=42)\n",
        "model_bag.fit(X_train, y_train)\n",
        "y_pred_bag = model_bag.predict(X_test)\n",
        "model_details(\"Bagging\", y_test, y_pred_bag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIQrmcsbCRUP"
      },
      "outputs": [],
      "source": [
        "# prompt: store all the model perfomance in csv file for all the above models\n",
        "\n",
        "import csv\n",
        "\n",
        "def store_model_performance(filename, model_name, y_true, y_pred):\n",
        "  \"\"\"Stores model performance metrics in a CSV file.\"\"\"\n",
        "\n",
        "  metrics = calculate_all_metrics(y_true, y_pred)\n",
        "\n",
        "  with open(filename, 'a', newline='') as csvfile:\n",
        "    fieldnames = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'MCC', 'Balanced Accuracy', 'Kappa']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header only if the file is newly created\n",
        "    if csvfile.tell() == 0:\n",
        "      writer.writeheader()\n",
        "\n",
        "    writer.writerow({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': metrics['Accuracy'],\n",
        "        'Precision': metrics['Precision'],\n",
        "        'Recall': metrics['Recall'],\n",
        "        'F1-Score': metrics['F1-Score'],\n",
        "        'ROC AUC': metrics['ROC AUC'],\n",
        "        'MCC': metrics['MCC'],\n",
        "        'Balanced Accuracy': metrics['Balanced Accuracy'],\n",
        "        'Kappa': metrics['Kappa']\n",
        "    })\n",
        "\n",
        "# Create a CSV file to store the results\n",
        "csv_filename = 'model_performance_morgan.csv'\n",
        "\n",
        "# Store the performance of each model\n",
        "store_model_performance(csv_filename, \"Random Forest\", y_test, y_pred)\n",
        "store_model_performance(csv_filename, \"Logistic Regression\", y_test, y_pred_lr)\n",
        "store_model_performance(csv_filename, \"SVM\", y_test, y_pred_svm)\n",
        "store_model_performance(csv_filename, \"KNN\", y_test, y_pred_knn)\n",
        "store_model_performance(csv_filename, \"MLP\", y_test, y_pred_mlp)\n",
        "store_model_performance(csv_filename, \"XGBoost\", y_test, y_pred_xgb)\n",
        "store_model_performance(csv_filename, \"Decision Tree\", y_test, y_pred_dt)\n",
        "store_model_performance(csv_filename, \"Naive Bayes\", y_test, y_pred_nb)\n",
        "store_model_performance(csv_filename, \"AdaBoost\", y_test, y_pred_ada)\n",
        "store_model_performance(csv_filename, \"Gradient Boosting\", y_test, y_pred_gb)\n",
        "store_model_performance(csv_filename, \"Bagging\", y_test, y_pred_bag)\n",
        "\n",
        "print(f\"Model performance saved to '{csv_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X5fTkylfs7x"
      },
      "outputs": [],
      "source": [
        "# prompt: add more machine learning\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "model_gb = GradientBoostingClassifier(random_state=42)\n",
        "model_gb.fit(X_train, y_train)\n",
        "y_pred_gb = model_gb.predict(X_test)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(classification_report(y_test, y_pred_gb))\n",
        "\n",
        "# Train a Multi-Layer Perceptron (MLP) model\n",
        "model_mlp = MLPClassifier(random_state=42)\n",
        "model_mlp.fit(X_train, y_train)\n",
        "y_pred_mlp = model_mlp.predict(X_test)\n",
        "print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))\n",
        "\n",
        "\n",
        "# Train a Decision Tree model\n",
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "model_dt.fit(X_train, y_train)\n",
        "y_pred_dt = model_dt.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "\n",
        "# Example usage for each model:\n",
        "model_details(\"Random Forest\", y_test, y_pred)\n",
        "model_details(\"Logistic Regression\", y_test, y_pred_lr)\n",
        "model_details(\"SVM\", y_test, y_pred_svm)\n",
        "model_details(\"KNN\", y_test, y_pred_knn)\n",
        "model_details(\"Gradient Boosting\", y_test, y_pred_gb)\n",
        "model_details(\"MLP\", y_test, y_pred_mlp)\n",
        "model_details(\"Decision Tree\", y_test, y_pred_dt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLv4TPsch5Fh"
      },
      "outputs": [],
      "source": [
        "# prompt: Used now the MACC fringerprint and use this metris : use this metrics as well : Accuracy,\n",
        "#         'Precision,\n",
        "#         'Recall,\n",
        "#         'F1-Score',\n",
        "#         'ROC AUC',\n",
        "#         'MCC,\n",
        "#         'Balanced Accuracy and Kappa\n",
        "\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import MACCSkeys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "from pycm import ConfusionMatrix\n",
        "\n",
        "# ... (Your existing code for data preparation and feature extraction) ...\n",
        "\n",
        "# Generate MACCS fingerprints\n",
        "def generate_maccs_fingerprint(smiles):\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  if mol is not None:\n",
        "    return MACCSkeys.GenMACCSKeys(mol)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "combined['MACCS_Fingerprint'] = combined['Smiles'].apply(generate_maccs_fingerprint)\n",
        "\n",
        "\n",
        "# Convert fingerprints to a suitable format for machine learning\n",
        "X_maccs = np.array([list(fp) for fp in combined['MACCS_Fingerprint'] if fp is not None])\n",
        "y = combined['label']\n",
        "\n",
        "# Handle missing values (if any)\n",
        "X_maccs = np.nan_to_num(X_maccs)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_maccs, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model\n",
        "model_rf_maccs = RandomForestClassifier(random_state=42)\n",
        "model_rf_maccs.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf_maccs = model_rf_maccs.predict(X_test)\n",
        "\n",
        "# Evaluate the model and calculate metrics\n",
        "def calculate_all_metrics(y_true, y_pred):\n",
        "  \"\"\"Calculates various classification metrics.\"\"\"\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "  cm = ConfusionMatrix(actual_vector=y_true.to_numpy(), predict_vector=y_pred)\n",
        "\n",
        "  metrics = {\n",
        "      'Accuracy': accuracy,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "      'F1-Score': f1,\n",
        "      'ROC AUC': roc_auc,\n",
        "      'MCC': mcc,\n",
        "      'Balanced Accuracy': balanced_accuracy,\n",
        "      'Kappa': kappa,\n",
        "      'Confusion Matrix': cm\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "\n",
        "metrics_rf_maccs = calculate_all_metrics(y_test, y_pred_rf_maccs)\n",
        "\n",
        "for metric_name, metric_value in metrics_rf_maccs.items():\n",
        "  print(f\"{metric_name}: {metric_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcJFaeBwJDPi"
      },
      "outputs": [],
      "source": [
        "# prompt: perform for Rf, KNN, MLP, Xgboost , Adaboost, decision tree, SVM, Gradient boosting, naive bayes, Logistic Regression, Bagging\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "from pycm import ConfusionMatrix\n",
        "\n",
        "\n",
        "# ... (Your existing code for data preparation and feature extraction) ...\n",
        "\n",
        "# Handle missing values\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  metrics = calculate_all_metrics(y_test, y_pred)\n",
        "  results[model_name] = metrics\n",
        "\n",
        "# Print and store the results\n",
        "for model_name, metrics in results.items():\n",
        "  print(f\"\\n--- {model_name} Model Details ---\")\n",
        "  for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")\n",
        "    # Store results in a CSV file (as before)\n",
        "    csv_filename = 'model_performance_MACCS.csv'\n",
        "    store_model_performance(csv_filename, model_name, y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Model performance saved to '{csv_filename}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBrM1ZfHMrM1"
      },
      "outputs": [],
      "source": [
        "# prompt: Please store the  performance of all MLS-MACCS\n",
        "\n",
        "# Assuming 'results' dictionary contains the model performance metrics from the previous code\n",
        "\n",
        "# Create a CSV file to store the results\n",
        "csv_filename = 'model_performance_MLS-MACCS.csv'\n",
        "\n",
        "# Store the performance of each model\n",
        "for model_name, metrics in results.items():\n",
        "  with open(csv_filename, 'a', newline='') as csvfile:\n",
        "    fieldnames = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'MCC', 'Balanced Accuracy', 'Kappa']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header only if the file is newly created\n",
        "    if csvfile.tell() == 0:\n",
        "      writer.writeheader()\n",
        "\n",
        "    writer.writerow({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': metrics['Accuracy'],\n",
        "        'Precision': metrics['Precision'],\n",
        "        'Recall': metrics['Recall'],\n",
        "        'F1-Score': metrics['F1-Score'],\n",
        "        'ROC AUC': metrics['ROC AUC'],\n",
        "        'MCC': metrics['MCC'],\n",
        "        'Balanced Accuracy': metrics['Balanced Accuracy'],\n",
        "        'Kappa': metrics['Kappa']\n",
        "    })\n",
        "\n",
        "print(f\"Model performance saved to '{csv_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4yv7f5IJpY1"
      },
      "outputs": [],
      "source": [
        "# prompt: use the  2D atom pair with all the above MLs\n",
        "\n",
        "import numpy as np\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "\n",
        "# ... (Your existing code for data preparation and feature extraction) ...\n",
        "\n",
        "# Generate 2D Atom Pair Fingerprints\n",
        "def generate_atompair_fingerprint(smiles):\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  if mol is not None:\n",
        "    return rdMolDescriptors.GetHashedAtomPairFingerprintAsBitVect(mol)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "combined['AtomPair_Fingerprint'] = combined['Smiles'].apply(generate_atompair_fingerprint)\n",
        "\n",
        "\n",
        "# Convert fingerprints to a suitable format for machine learning\n",
        "X_atompair = np.array([list(fp) for fp in combined['AtomPair_Fingerprint'] if fp is not None])\n",
        "y = combined['label']\n",
        "\n",
        "# Handle missing values (if any)\n",
        "X_atompair = np.nan_to_num(X_atompair)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_atompair, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  metrics = calculate_all_metrics(y_test, y_pred)\n",
        "  results[model_name] = metrics\n",
        "\n",
        "# Print and store the results\n",
        "for model_name, metrics in results.items():\n",
        "  print(f\"\\n--- {model_name} Model Details ---\")\n",
        "  for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")\n",
        "    # Store results in a CSV file (as before)\n",
        "    csv_filename = 'model_performance_atompair.csv'\n",
        "    store_model_performance(csv_filename, model_name, y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Model performance saved to '{csv_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRogihx1uCNF"
      },
      "outputs": [],
      "source": [
        "! pip install padelpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1JtM-SBzJ4B",
        "outputId": "ad7b92c4-1a56-4a79-82d0-a5f74ad3cf90"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-02 18:50:27--  https://github.com/dataprofessor/padel/raw/main/fingerprints_xml.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dataprofessor/padel/main/fingerprints_xml.zip [following]\n",
            "--2024-10-02 18:50:28--  https://raw.githubusercontent.com/dataprofessor/padel/main/fingerprints_xml.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10871 (11K) [application/zip]\n",
            "Saving to: fingerprints_xml.zip.1\n",
            "\n",
            "fingerprints_xml.zi 100%[===================>]  10.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-10-02 18:50:28 (69.8 MB/s) - fingerprints_xml.zip.1 saved [10871/10871]\n",
            "\n",
            "Archive:  fingerprints_xml.zip\n",
            "replace AtomPairs2DFingerprintCount.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "! wget https://github.com/dataprofessor/padel/raw/main/fingerprints_xml.zip\n",
        "! unzip fingerprints_xml.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVYWYkQz1Z5k"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "xml_files = glob.glob(\"*.xml\")\n",
        "xml_files.sort()\n",
        "xml_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIAaG1GQ2d-u"
      },
      "outputs": [],
      "source": [
        "FP_list = ['AtomPairs2DCount',\n",
        " 'AtomPairs2D',\n",
        " 'EState',\n",
        " 'CDKextended',\n",
        " 'CDK',\n",
        " 'CDKgraphonly',\n",
        " 'KlekotaRothCount',\n",
        " 'KlekotaRoth',\n",
        " 'MACCS',\n",
        " 'PubChem',\n",
        " 'SubstructureCount',\n",
        " 'Substructure']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL3YfAxu2d8G"
      },
      "outputs": [],
      "source": [
        "fp = dict(zip(FP_list, xml_files))\n",
        "fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4jJy-YM2d5e"
      },
      "outputs": [],
      "source": [
        "df = combined\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBo8SLFDOjKL"
      },
      "outputs": [],
      "source": [
        "new_df\n",
        "df3 = new_df\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otB_WaoOOwVf"
      },
      "outputs": [],
      "source": [
        "# prompt: change the name of Standardized_Smiles to Smiles\n",
        "\n",
        "df3.rename(columns={'Standardized_Smiles': 'Smiles'}, inplace=True)\n",
        "df3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taBoxF0ePspF"
      },
      "outputs": [],
      "source": [
        "df2 = pd.concat( [df3['Smiles'],df3['Molecule ChEMBL ID']], axis=1 )\n",
        "df2.to_csv('molecule.smi', sep='\\t', index=False, header=False)\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbB_v-522d0L"
      },
      "outputs": [],
      "source": [
        "from padelpy import padeldescriptor\n",
        "\n",
        "fingerprint = 'MACCS'\n",
        "\n",
        "fingerprint_output_file_MACCS = ''.join([fingerprint,'.csv']) #MACCS.csv\n",
        "fingerprint_descriptortypes = fp[fingerprint]\n",
        "\n",
        "padeldescriptor(mol_dir='molecule.smi',\n",
        "                d_file=fingerprint_output_file_MACCS, #'MACCS.csv'\n",
        "                #descriptortypes='MACCSFingerprinter.xml',\n",
        "                descriptortypes= fingerprint_descriptortypes,\n",
        "                detectaromaticity=True,\n",
        "                standardizenitro=True,\n",
        "                standardizetautomers=True,\n",
        "                threads=2,\n",
        "                removesalt=True,\n",
        "                log=True,\n",
        "                fingerprints=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu-nnJRoGE1P"
      },
      "source": [
        "MACSS MAchine learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TBCGynX2dtx"
      },
      "outputs": [],
      "source": [
        "descriptors_MACCS = pd.read_csv(fingerprint_output_file_MACCS)\n",
        "descriptors_MACCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4T4fDTDFiaD"
      },
      "outputs": [],
      "source": [
        "df4= df3['label']\n",
        "df4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHjGkjcaFt9D"
      },
      "outputs": [],
      "source": [
        "df1= pd.concat([descriptors_MACCS, df3['label']], axis=1)\n",
        "df1 = df1.drop('Name', axis=1)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGIryo_KHJw-"
      },
      "outputs": [],
      "source": [
        "# prompt: build 10 MLs classifies modeld with df1 data and with the metrics used preciously\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "from pycm import ConfusionMatrix\n",
        "import csv\n",
        "\n",
        "# Prepare your data\n",
        "X = df1.drop('label', axis=1)\n",
        "y = df1['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  metrics = calculate_all_metrics(y_test, y_pred)\n",
        "  results[model_name] = metrics\n",
        "\n",
        "# Print and store the results\n",
        "for model_name, metrics in results.items():\n",
        "  print(f\"\\n--- {model_name} Model Details ---\")\n",
        "  for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")\n",
        "    # Store results in a CSV file (as before)\n",
        "    csv_filename = 'model_performance_MACCS_MLS2.csv'\n",
        "    store_model_performance(csv_filename, model_name, y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Model performance saved to '{csv_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQapPvkcI406"
      },
      "outputs": [],
      "source": [
        "# prompt: provides the metrics for the testing dataset as independent dataset. for the above Mls models\n",
        "\n",
        "# Assuming you have already trained your models and have X_test and y_test\n",
        "\n",
        "# Create an empty dictionary to store the metrics for each model on the test data\n",
        "test_results = {}\n",
        "\n",
        "# Loop through each model and make predictions on the test data\n",
        "for model_name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the metrics for the test data\n",
        "    metrics = calculate_all_metrics(y_test, y_pred)\n",
        "    test_results[model_name] = metrics\n",
        "\n",
        "# Print and store the results for each model\n",
        "for model_name, metrics in test_results.items():\n",
        "    print(f\"\\n--- {model_name} Model Details (Test Data) ---\")\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "        print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "    # Store results in a CSV file (as before)\n",
        "    csv_filename = 'model_performance_MACCS_MLS2_test.csv'\n",
        "    store_model_performance(csv_filename, model_name, y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Model performance (Test Data) saved to '{csv_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFZH51TnIreI"
      },
      "outputs": [],
      "source": [
        "# prompt: check the overfit of the developped models\n",
        "\n",
        "# Assuming you have already trained your models and have X_train, y_train, X_test, y_test\n",
        "\n",
        "# Create an empty dictionary to store the metrics for each model on the training and testing data\n",
        "train_test_results = {}\n",
        "\n",
        "# Loop through each model and make predictions on the training and testing data\n",
        "for model_name, model in models.items():\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the metrics for the training and testing data\n",
        "    train_metrics = calculate_all_metrics(y_train, y_train_pred)\n",
        "    test_metrics = calculate_all_metrics(y_test, y_test_pred)\n",
        "    train_test_results[model_name] = {'train': train_metrics, 'test': test_metrics}\n",
        "\n",
        "# Print and store the results for each model\n",
        "for model_name, metrics in train_test_results.items():\n",
        "    print(f\"\\n--- {model_name} Model Overfit Analysis ---\")\n",
        "    print(\"Training Data Metrics:\")\n",
        "    for metric_name, metric_value in metrics['train'].items():\n",
        "        print(f\"{metric_name}: {metric_value}\")\n",
        "    print(\"\\nTesting Data Metrics:\")\n",
        "    for metric_name, metric_value in metrics['test'].items():\n",
        "        print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "    # You can compare the training and testing metrics to identify potential overfitting\n",
        "    # For example, if the training accuracy is significantly higher than the testing accuracy,\n",
        "    # it could indicate overfitting.\n",
        "\n",
        "\n",
        "# Optional: Store the train and test metrics in a CSV file for further analysis\n",
        "# You can modify this part to include the desired columns and format\n",
        "csv_filename = 'model_overfit_analysis.csv'\n",
        "with open(csv_filename, 'w', newline='') as csvfile:\n",
        "    fieldnames = ['Model', 'Data_Split', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'MCC', 'Balanced Accuracy', 'Kappa']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for model_name, metrics in train_test_results.items():\n",
        "        for data_split, metric_dict in metrics.items():\n",
        "            writer.writerow({\n",
        "                'Model': model_name,\n",
        "                'Data_Split': data_split,\n",
        "                'Accuracy': metric_dict['Accuracy'],\n",
        "                'Precision': metric_dict['Precision'],\n",
        "                'Recall': metric_dict['Recall'],\n",
        "                'F1-Score': metric_dict['F1-Score'],\n",
        "                'ROC AUC': metric_dict['ROC AUC'],\n",
        "                'MCC': metric_dict['MCC'],\n",
        "                'Balanced Accuracy': metric_dict['Balanced Accuracy'],\n",
        "                'Kappa': metric_dict['Kappa']\n",
        "            })\n",
        "\n",
        "print(f\"Model Overfit Analysis saved to '{csv_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NQdgMkY0xhT"
      },
      "outputs": [],
      "source": [
        "# prompt: calculate for the following fingerprint :'AtomPairs2DCount', 'AtomPairs2D', 'EState', 'CDKextended', 'CDK', 'CDKgraphonly' save the output for each descriptor in dataframe\n",
        "\n",
        "import pandas as pd\n",
        "fingerprint_list = ['AtomPairs2DCount', 'AtomPairs2D', 'EState', 'CDKextended', 'CDK', 'CDKgraphonly']\n",
        "\n",
        "for fingerprint in fingerprint_list:\n",
        "  fingerprint_output_file = ''.join([fingerprint,'.csv'])\n",
        "  fingerprint_descriptortypes = fp[fingerprint]\n",
        "\n",
        "  padeldescriptor(mol_dir='molecule.smi',\n",
        "                  d_file=fingerprint_output_file,\n",
        "                  descriptortypes=fingerprint_descriptortypes,\n",
        "                  detectaromaticity=True,\n",
        "                  standardizenitro=True,\n",
        "                  standardizetautomers=True,\n",
        "                  threads=2,\n",
        "                  removesalt=True,\n",
        "                  log=True,\n",
        "                  fingerprints=True)\n",
        "\n",
        "  descriptors = pd.read_csv(fingerprint_output_file)\n",
        "  # You can now process or store the descriptors dataframe as needed\n",
        "  print(f\"Descriptors for {fingerprint} calculated and saved to {fingerprint_output_file}\")\n",
        "  # Example: store the dataframe in a dictionary or list\n",
        "  #your_dataframe_dict[fingerprint] = descriptors\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qqnNO8KKRc"
      },
      "source": [
        "Machine learning for AtoMPaird"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdR2-_4S234Q"
      },
      "outputs": [],
      "source": [
        "descriptors_AtomPairs2D = pd.read_csv('AtomPairs2D.csv')\n",
        "descriptors_AtomPairs2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e17i8zbJKTWC"
      },
      "outputs": [],
      "source": [
        "df4= df3['label']\n",
        "df4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9UuGv6QKYr5"
      },
      "outputs": [],
      "source": [
        "df2 =pd.concat([descriptors_AtomPairs2D, df3['label']], axis=1)\n",
        "df2 = df2.drop('Name', axis=1)\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z29i3WAoKh3R"
      },
      "outputs": [],
      "source": [
        "# prompt: build 10 MLs classifies modeld with df1 data and with the metrics used preciously\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "from pycm import ConfusionMatrix\n",
        "\n",
        "# Assuming df1 is your dataframe with features (X) and labels (y)\n",
        "X = df1.drop('label', axis=1)\n",
        "y = df1['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define your models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Function to calculate metrics\n",
        "def calculate_all_metrics(y_true, y_pred):\n",
        "  \"\"\"Calculates various classification metrics.\"\"\"\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "  cm = ConfusionMatrix(actual_vector=y_true.to_numpy(), predict_vector=y_pred)\n",
        "\n",
        "  metrics = {\n",
        "      'Accuracy': accuracy,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "      'F1-Score': f1,\n",
        "      'ROC AUC': roc_auc,\n",
        "      'MCC': mcc,\n",
        "      'Balanced Accuracy': balanced_accuracy,\n",
        "      'Kappa': kappa,\n",
        "      'Confusion Matrix': cm\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  metrics = calculate_all_metrics(y_test, y_pred)\n",
        "  results[model_name] = metrics\n",
        "\n",
        "# Print results\n",
        "for model_name, metrics in results.items():\n",
        "  print(f\"\\n--- {model_name} Model Details ---\")\n",
        "  for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7G_0Eak3FEJ"
      },
      "outputs": [],
      "source": [
        "Descriptors_AtomPairs2DCounts = pd.read_csv('AtomPairs2DCount.csv')\n",
        "Descriptors_AtomPairs2DCounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHzgUl113Rhl"
      },
      "outputs": [],
      "source": [
        "Descriptors_EState = pd.read_csv('EState.csv')\n",
        "Descriptors_EState"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz0dJZt-3eT3"
      },
      "outputs": [],
      "source": [
        "Descriptors_CDKextended = pd.read_csv('CDKextended.csv')\n",
        "Descriptors_CDKextended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG5sxGQ63i7B"
      },
      "outputs": [],
      "source": [
        "Descriptors_CDK = pd.read_csv('CDK.csv')\n",
        "Descriptors_CDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCRGqZ9p4Kd6"
      },
      "outputs": [],
      "source": [
        "# prompt: calculate for the following fingerprint :'AtomPairs2DCount', 'AtomPairs2D', 'EState', 'CDKextended', 'CDK', 'CDKgraphonly' save the output for each descriptor in dataframe\n",
        "\n",
        "import pandas as pd\n",
        "fingerprint_list = ['PubChem','SubstructureCount','Substructure']\n",
        "\n",
        "for fingerprint in fingerprint_list:\n",
        "  fingerprint_output_file = ''.join([fingerprint,'.csv'])\n",
        "  fingerprint_descriptortypes = fp[fingerprint]\n",
        "\n",
        "  padeldescriptor(mol_dir='molecule.smi',\n",
        "                  d_file=fingerprint_output_file,\n",
        "                  descriptortypes=fingerprint_descriptortypes,\n",
        "                  detectaromaticity=True,\n",
        "                  standardizenitro=True,\n",
        "                  standardizetautomers=True,\n",
        "                  threads=2,\n",
        "                  removesalt=True,\n",
        "                  log=True,\n",
        "                  fingerprints=True)\n",
        "\n",
        "  descriptors = pd.read_csv(fingerprint_output_file)\n",
        "  # You can now process or store the descriptors dataframe as needed\n",
        "  print(f\"Descriptors for {fingerprint} calculated and saved to {fingerprint_output_file}\")\n",
        "  # Example: store the dataframe in a dictionary or list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUh2pyRmJkm"
      },
      "outputs": [],
      "source": [
        " # prompt: output for each fingerprint\n",
        "\n",
        "import pandas as pd\n",
        "# Assuming 'combined' DataFrame and 'new_df' DataFrame are already defined\n",
        "\n",
        "# List of fingerprints to calculate\n",
        "fingerprint_list = ['KlekotaRothCount', 'KlekotaRoth']\n",
        "\n",
        "# Create a dictionary to map fingerprint names to XML files\n",
        "fp = dict(zip(fingerprint_list, xml_files))\n",
        "\n",
        "# Iterate thr#ough the fingerprints and calculate them\n",
        "for fingerprint in fingerprint_list:\n",
        "  fingerprint_output_file = ''.join([fingerprint, '.csv'])\n",
        "  fingerprint_descriptortypes = fp[fingerprint]\n",
        "\n",
        "  df2 = pd.concat([new_df['Smiles'], new_df['Molecule ChEMBL ID']], axis=1)\n",
        "  df2.to_csv('molecule.smi', sep='\\t', index=False, header=False)\n",
        "\n",
        "  padeldescriptor(mol_dir='molecule.smi',\n",
        "                  d_file=fingerprint_output_file,\n",
        "                  descriptortypes=fingerprint_descriptortypes,\n",
        "                  detectaromaticity=True,\n",
        "                  standardizenitro=True,\n",
        "                  standardizetautomers=True,\n",
        "                  threads=2,\n",
        "                  removesalt=True,\n",
        "                  log=True,\n",
        "                  fingerprints=True)\n",
        "\n",
        "  descriptors = pd.read_csv(fingerprint_output_file)\n",
        "  print(f\"\\n--- {fingerprint} Fingerprint Output ---\")\n",
        "  print(descriptors.head())  # Print the first few rows of the calculated descriptors\n",
        "  # Process the calculated descriptors (e.g., merge with your original DataFrame)\n",
        "  # ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e51Z1A2XbjN"
      },
      "outputs": [],
      "source": [
        "# prompt: store the output of the calculation in dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have already calculated the descriptors and they are stored in a CSV file named 'fingerprint_output_file'\n",
        "\n",
        "# Create an empty dictionary to store the dataframes\n",
        "descriptors_dict = {}\n",
        "\n",
        "# Iterate through the fingerprints and load the descriptors into the dictionary\n",
        "for fingerprint in fingerprint_list:\n",
        "  fingerprint_output_file = ''.join([fingerprint,'.csv'])\n",
        "  try:\n",
        "    descriptors_dict[fingerprint] = pd.read_csv(fingerprint_output_file)\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found for {fingerprint}: {fingerprint_output_file}\")\n",
        "\n",
        "# You can access the dataframes using the fingerprint names as keys in the dictionary\n",
        "# For example, to access the dataframe for 'MACCS':\n",
        "# descriptors_dict['MACCS']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fH4agcAXmyZ"
      },
      "outputs": [],
      "source": [
        "Descriptors_KlekotaRoth= descriptors_dict['KlekotaRoth']\n",
        "Descriptors_KlekotaRoth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcCkfoGZl9S7"
      },
      "outputs": [],
      "source": [
        "Descriptors_KlekotaRoth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9EX6nqR9PsO"
      },
      "outputs": [],
      "source": [
        "# prompt: from all the calculated AtomPairs2D',\n",
        "# #  'EState',\n",
        "# #  'CDKextended',\n",
        "# #  'CDK',\n",
        "# #  'CDKgraphonly',\n",
        "# #  'KlekotaRothCount',\n",
        "# #  'KlekotaRoth',\n",
        "# #  'MACCS',\n",
        "# #  'PubChem',\n",
        "# #  'SubstructureCount',\n",
        "# #  'Substructure descriptors add in each new colunm label from df4 and store as dataframe\n",
        "\n",
        "import pandas as pd\n",
        "fingerprint_list = ['AtomPairs2D', 'EState', 'CDKextended', 'CDK', 'CDKgraphonly',\n",
        "                    'KlekotaRothCount', 'KlekotaRoth', 'MACCS', 'PubChem',\n",
        "                    'SubstructureCount', 'Substructure']\n",
        "\n",
        "# Create a dictionary to store DataFrames\n",
        "descriptors_dfs = {}\n",
        "\n",
        "# Loop through the list of fingerprints\n",
        "for fingerprint in fingerprint_list:\n",
        "  fingerprint_output_file = ''.join([fingerprint,'.csv'])  # Construct the file name\n",
        "\n",
        "  try:\n",
        "    descriptors_df = pd.read_csv(fingerprint_output_file)\n",
        "    descriptors_df = pd.concat([descriptors_df, df4], axis=1)  # Add the 'label' column from df4\n",
        "    descriptors_dfs[fingerprint] = descriptors_df  # Store the DataFrame in the dictionary\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not found for {fingerprint}: {fingerprint_output_file}\")\n",
        "\n",
        "# Now you have a dictionary 'descriptors_dfs' where the keys are the fingerprint names\n",
        "# and the values are the corresponding DataFrames with 'label' added.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_9k9cVB9PpP"
      },
      "outputs": [],
      "source": [
        "# prompt: print them\n",
        "\n",
        "for fingerprint, df in descriptors_dfs.items():\n",
        "  print(f\"\\n--- {fingerprint} Fingerprint DataFrame ---\")\n",
        "  print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCeqxC2v9PmP"
      },
      "outputs": [],
      "source": [
        "# prompt: in all the developed Dataframe, remove the colunm:\"Name\" and print them\n",
        "\n",
        "for fingerprint, df in descriptors_dfs.items():\n",
        "  if 'Name' in df.columns:\n",
        "    df = df.drop('Name', axis=1)\n",
        "    print(f\"\\n--- {fingerprint} Fingerprint DataFrame (Name column removed) ---\")\n",
        "    print(df.head())\n",
        "  else:\n",
        "    print(f\"\\n--- {fingerprint} Fingerprint DataFrame (Name column not found) ---\")\n",
        "    print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ociZoaed9Pgg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnyVVbFN9Pdp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptjx4XHq9PaZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXnuWH3Bl2La"
      },
      "outputs": [],
      "source": [
        "# prompt: print descriptor foe each fingerprint\n",
        "\n",
        "# Assuming 'descriptors_MACCS' is already defined\n",
        "print(\"MACCS Descriptors:\")\n",
        "print(descriptors_MACCS.head())  # Print the first few rows of MACCS descriptors\n",
        "\n",
        "# Assuming you have other descriptor DataFrames (e.g., descriptors_AtomPairs2D)\n",
        "# Print descriptors for each calculated fingerprint\n",
        "# for example\n",
        "# print(\"AtomPairs2D Descriptors:\")\n",
        "# print(descriptors_AtomPairs2D.head())\n",
        "\n",
        "# You can continue this pattern for all the fingerprint descriptors you calculated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjM-ENqZIPRu"
      },
      "outputs": [],
      "source": [
        "# prompt: for each developed dataframe name them descriptor_fringerprint\n",
        "\n",
        "# Create a dictionary to store DataFrames with descriptive names\n",
        "descriptors_dfs_renamed = {}\n",
        "\n",
        "# Loop through the list of fingerprints and rename DataFrames\n",
        "for fingerprint, df in descriptors_dfs.items():\n",
        "  if 'Name' in df.columns:\n",
        "    df = df.drop('Name', axis=1)\n",
        "  descriptors_dfs_renamed[f'descriptors_{fingerprint}'] = df\n",
        "\n",
        "# Now you have a dictionary 'descriptors_dfs_renamed' where the keys are like 'descriptors_MACCS',\n",
        "# 'descriptors_AtomPairs2D', etc., and the values are the corresponding DataFrames.\n",
        "\n",
        "# You can access the DataFrames using these keys:\n",
        "# Example:\n",
        "# descriptors_MACCS_df = descriptors_dfs_renamed['descriptors_MACCS']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sVigko9IdT4"
      },
      "outputs": [],
      "source": [
        "# prompt: print all the dataframe\n",
        "\n",
        "for df_name, df in descriptors_dfs_renamed.items():\n",
        "  print(f\"\\n--- {df_name} DataFrame ---\")\n",
        "  print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT5a0o_VIlHp"
      },
      "outputs": [],
      "source": [
        "# prompt: for each descriptors dataframe, build  12MLs, provide all the above metrics. Please each model must be name ML-fringerprint.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "from pycm import ConfusionMatrix\n",
        "import csv\n",
        "\n",
        "# Assuming descriptors_dfs_renamed is your dictionary of DataFrames\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred):\n",
        "  \"\"\"Calculates various classification metrics.\"\"\"\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "  cm = ConfusionMatrix(actual_vector=y_true.to_numpy(), predict_vector=y_pred)\n",
        "\n",
        "  metrics = {\n",
        "      'Accuracy': accuracy,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "      'F1-Score': f1,\n",
        "      'ROC AUC': roc_auc,\n",
        "      'MCC': mcc,\n",
        "      'Balanced Accuracy': balanced_accuracy,\n",
        "      'Kappa': kappa,\n",
        "      'Confusion Matrix': cm\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "# Define your models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Loop through each DataFrame in the dictionary\n",
        "for df_name, df in descriptors_dfs_renamed.items():\n",
        "  print(f\"\\n--- Building ML Models for {df_name} ---\")\n",
        "\n",
        "  # Split data into features (X) and labels (y)\n",
        "  X = df.drop('label', axis=1)\n",
        "  y = df['label']\n",
        "\n",
        "  # Split data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Train and evaluate each model\n",
        "  for model_name, model in models.items():\n",
        "    model_fingerprint_name = f\"{model_name}-{df_name}\"  # Create a unique name for each model\n",
        "    print(f\"\\nTraining {model_fingerprint_name}\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = calculate_all_metrics(y_test, y_pred)\n",
        "\n",
        "    print(f\"--- {model_fingerprint_name} Metrics ---\")\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "      print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "    # Store the results in a CSV file (optional)\n",
        "    csv_filename = 'model_performance_ML_fingerprints.csv'\n",
        "    with open(csv_filename, 'a', newline='') as csvfile:\n",
        "      fieldnames = ['Model', 'Fingerprint', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'MCC', 'Balanced Accuracy', 'Kappa']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      if not csvfile.tell():  # Write header only if the file is empty\n",
        "        writer.writeheader()\n",
        "\n",
        "      writer.writerow({\n",
        "          'Model': model_name,\n",
        "          'Fingerprint': df_name,\n",
        "          'Accuracy': metrics['Accuracy'],\n",
        "          'Precision': metrics['Precision'],\n",
        "          'Recall': metrics['Recall'],\n",
        "          'F1-Score': metrics['F1-Score'],\n",
        "          'ROC AUC': metrics['ROC AUC'],\n",
        "          'MCC': metrics['MCC'],\n",
        "          'Balanced Accuracy': metrics['Balanced Accuracy'],\n",
        "          'Kappa': metrics['Kappa']\n",
        "      })\n",
        "\n",
        "print(f\"Model Performance (ML-Fingerprint) saved to '{csv_filename}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHerTe38b_qt"
      },
      "outputs": [],
      "source": [
        "# prompt: perform individual model analysis\n",
        "\n",
        "import pandas as pd\n",
        "# Assuming descriptors_dfs_renamed is your dictionary of DataFrames\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred):\n",
        "  \"\"\"Calculates various classification metrics.\"\"\"\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "  cm = ConfusionMatrix(actual_vector=y_true.to_numpy(), predict_vector=y_pred)\n",
        "\n",
        "  metrics = {\n",
        "      'Accuracy': accuracy,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "      'F1-Score': f1,\n",
        "      'ROC AUC': roc_auc,\n",
        "      'MCC': mcc,\n",
        "      'Balanced Accuracy': balanced_accuracy,\n",
        "      'Kappa': kappa,\n",
        "      'Confusion Matrix': cm\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "# Define your models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Loop through each DataFrame in the dictionary\n",
        "for df_name, df in descriptors_dfs_renamed.items():\n",
        "  print(f\"\\n--- Individual Model Analysis for {df_name} ---\")\n",
        "\n",
        "  # Split data into features (X) and labels (y)\n",
        "  X = df.drop('label', axis=1)\n",
        "  y = df['label']\n",
        "\n",
        "  # Split data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Train and evaluate each model\n",
        "  for model_name, model in models.items():\n",
        "    model_fingerprint_name = f\"{model_name}-{df_name}\"  # Create a unique name for each model\n",
        "    print(f\"\\nAnalyzing {model_fingerprint_name}\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = calculate_all_metrics(y_test, y_pred)\n",
        "\n",
        "    print(f\"--- {model_fingerprint_name} Metrics ---\")\n",
        "    for metric_name, metric_value in metrics.items():\n",
        "      print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "    # Individual model analysis (example: feature importance for Random Forest)\n",
        "    if model_name == 'Random Forest':\n",
        "      try:\n",
        "        importances = model.feature_importances_\n",
        "        feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
        "        feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
        "        print(\"\\n--- Top 10 Important Features for Random Forest ---\")\n",
        "        print(feature_importance_df.head(10))\n",
        "      except Exception as e:\n",
        "        print(f\"Error calculating feature importance for Random Forest: {e}\")\n",
        "\n",
        "    # Add more analysis for other models as needed (e.g., coefficients for Logistic Regression, etc.)\n",
        "\n",
        "print(\"Individual Model Analysis completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPTO5mpab_nB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# prompt: perform 10 fold cross validation to enchance the individual model performance and output for the metrics\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Assuming descriptors_dfs_renamed is your dictionary of DataFrames\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred):\n",
        "  \"\"\"Calculates various classification metrics.\"\"\"\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "  cm = ConfusionMatrix(actual_vector=y_true.to_numpy(), predict_vector=y_pred)\n",
        "\n",
        "  metrics = {\n",
        "      'Accuracy': accuracy,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "      'F1-Score': f1,\n",
        "      'ROC AUC': roc_auc,\n",
        "      'MCC': mcc,\n",
        "      'Balanced Accuracy': balanced_accuracy,\n",
        "      'Kappa': kappa,\n",
        "      'Confusion Matrix': cm\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "# Define your models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'MLP': MLPClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Bagging': BaggingClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Loop through each DataFrame in the dictionary\n",
        "for df_name, df in descriptors_dfs_renamed.items():\n",
        "  print(f\"\\n--- 10-Fold Cross-Validation for {df_name} ---\")\n",
        "\n",
        "  # Split data into features (X) and labels (y)\n",
        "  X = df.drop('label', axis=1)\n",
        "  y = df['label']\n",
        "\n",
        "  # Define the cross-validation strategy\n",
        "  cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "  # Train and evaluate each model using cross-validation\n",
        "  for model_name, model in models.items():\n",
        "    model_fingerprint_name = f\"{model_name}-{df_name}\"  # Create a unique name for each model\n",
        "    print(f\"\\nAnalyzing {model_fingerprint_name} with 10-Fold CV\")\n",
        "\n",
        "    # Use cross_val_score to perform cross-validation\n",
        "    accuracy_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "    precision_scores = cross_val_score(model, X, y, cv=cv, scoring='precision')\n",
        "    recall_scores = cross_val_score(model, X, y, cv=cv, scoring='recall')\n",
        "    f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1')\n",
        "    roc_auc_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')\n",
        "\n",
        "\n",
        "    print(f\"--- {model_fingerprint_name} Cross-Validation Metrics ---\")\n",
        "    print(f\"Accuracy: {accuracy_scores.mean():.4f} (+/- {accuracy_scores.std():.4f})\")\n",
        "    print(f\"Precision: {precision_scores.mean():.4f} (+/- {precision_scores.std():.4f})\")\n",
        "    print(f\"Recall: {recall_scores.mean():.4f} (+/- {recall_scores.std():.4f})\")\n",
        "    print(f\"F1-Score: {f1_scores.mean():.4f} (+/- {f1_scores.std():.4f})\")\n",
        "    print(f\"ROC AUC: {roc_auc_scores.mean():.4f} (+/- {roc_auc_scores.std():.4f})\")\n",
        "\n",
        "    # Store the results in a CSV file (optional)\n",
        "    csv_filename = 'model_performance_CV_ML_fingerprints.csv'\n",
        "    with open(csv_filename, 'a', newline='') as csvfile:\n",
        "      fieldnames = ['Model', 'Fingerprint', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "      if not csvfile.tell():  # Write header only if the file is empty\n",
        "        writer.writeheader()\n",
        "\n",
        "      writer.writerow({\n",
        "          'Model': model_name,\n",
        "          'Fingerprint': df_name,\n",
        "          'Accuracy': accuracy_scores.mean(),\n",
        "          'Precision': precision_scores.mean(),\n",
        "          'Recall': recall_scores.mean(),\n",
        "          'F1-Score': f1_scores.mean(),\n",
        "          'ROC AUC': roc_auc_scores.mean()\n",
        "      })\n",
        "\n",
        "print(f\"Model Performance (CV-ML-Fingerprint) saved to '{csv_filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZcGKzI7b_fl"
      },
      "outputs": [],
      "source": [
        "# prompt: add as well sensibility and specificity metrics\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import Lipinski\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from rdkit.Chem import rdmolops\n",
        "from scipy.stats import ttest_ind\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from rdkit.Chem.MolStandardize import rdMolStandardize # Import the required module\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from pycm import ConfusionMatrix\n",
        "import csv\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import matthews_corrcoef, balanced_accuracy_score, cohen_kappa_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "def calculate_all_metrics(y_true, y_pred):\n",
        "  \"\"\"Calculates various classification metrics.\"\"\"\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  mcc = matthews_corrcoef(y_true, y_pred)\n",
        "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "  cm = ConfusionMatrix(actual_vector=y_true.to_numpy(), predict_vector=y_pred)\n",
        "\n",
        "  # Calculate Sensitivity (Recall of the positive class)\n",
        "  try:\n",
        "      sensitivity = recall_score(y_true, y_pred, pos_label=1)\n",
        "  except ValueError:\n",
        "      sensitivity = 0.0\n",
        "\n",
        "  # Calculate Specificity (Recall of the negative class)\n",
        "  try:\n",
        "      specificity = recall_score(y_true, y_pred, pos_label=0)\n",
        "  except ValueError:\n",
        "      specificity = 0.0\n",
        "\n",
        "  metrics = {\n",
        "      'Accuracy': accuracy,\n",
        "      'Precision': precision,\n",
        "      'Recall': recall,\n",
        "      'F1-Score': f1,\n",
        "      'ROC AUC': roc_auc,\n",
        "      'MCC': mcc,\n",
        "      'Balanced Accuracy': balanced_accuracy,\n",
        "      'Kappa': kappa,\n",
        "      'Sensitivity': sensitivity,\n",
        "      'Specificity': specificity,\n",
        "      'Confusion Matrix': cm\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "\n",
        "# ... (Your existing code for model training and evaluation) ...\n",
        "\n",
        "# Inside your loop for each model:\n",
        "# ...\n",
        "metrics = calculate_all_metrics(y_test, y_pred)\n",
        "\n",
        "print(f\"--- {model_fingerprint_name} Metrics ---\")\n",
        "for metric_name, metric_value in metrics.items():\n",
        "  print(f\"{metric_name}: {metric_value}\")\n",
        "\n",
        "# ... (Rest of your code for storing results in CSV) ...\n",
        "\n",
        "# Example of how to update your CSV writer:\n",
        "csv_filename = 'model_performance_CV_ML_fingerprints.csv'\n",
        "with open(csv_filename, 'a', newline='') as csvfile:\n",
        "  fieldnames = ['Model', 'Fingerprint', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC', 'MCC', 'Balanced Accuracy', 'Kappa', 'Sensitivity', 'Specificity']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  if not csvfile.tell():  # Write header only if the file is empty\n",
        "    writer.writeheader()\n",
        "\n",
        "  writer.writerow({\n",
        "      'Model': model_name,\n",
        "      'Fingerprint': df_name,\n",
        "      'Accuracy': metrics['Accuracy'],\n",
        "      'Precision': metrics['Precision'],\n",
        "      'Recall': metrics['Recall'],\n",
        "      'F1-Score': metrics['F1-Score'],\n",
        "      'ROC AUC': metrics['ROC AUC'],\n",
        "      'MCC': metrics['MCC'],\n",
        "      'Balanced Accuracy': metrics['Balanced Accuracy'],\n",
        "      'Kappa': metrics['Kappa'],\n",
        "      'Sensitivity': metrics['Sensitivity'],\n",
        "      'Specificity': metrics['Specificity']\n",
        "  })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuWLfxun_gzB"
      },
      "outputs": [],
      "source": [
        "# prompt: can you number total number of baseline Mls developed\n",
        "\n",
        "total_models = len(models) * len(descriptors_dfs_renamed)\n",
        "print(f\"Total number of baseline ML models developed: {total_models}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orRVZ7bO2doH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JajdheiO2dle"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY2qKIni2di_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCMPKhP32dgH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hWpwZv-2dSY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPwkq-oU2dQP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSAojvgL2dO1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfqOKf5w2dLa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4TXm4Zu2dJH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_LMiRoM2dG5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21vRSneE2dEp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAsWAn012dCK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvOMRNua2c_s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWHfM_if2c85"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPWqlI+dkt25+W0yLkyJj35",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}